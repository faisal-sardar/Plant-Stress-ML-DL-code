import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, SimpleRNN, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.metrics import Precision, Recall
from sklearn.model_selection import train_test_split
import numpy as np
from google.colab import files
from sklearn.utils import shuffle
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
# Example: Custom F1-Score Metric
class F1Score(tf.keras.metrics.Metric):
    def __init__(self, name='f1_score', **kwargs):
        super(F1Score, self).__init__(name=name, **kwargs)
        self.precision = Precision()
        self.recall = Recall()

    def update_state(self, y_true, y_pred, sample_weight=None):
        self.precision.update_state(y_true, y_pred, sample_weight)
        self.recall.update_state(y_true, y_pred, sample_weight)

    def result(self):
        p = self.precision.result()
        r = self.recall.result()
        return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))

    def reset_states(self):
        self.precision.reset_states()
        self.recall.reset_states()

# Upload data
uploaded = files.upload()

# Print uploaded filenames
print("Uploaded filenames:", list(uploaded.keys()))

# Assuming data is in CSV format
import io
import pandas as pd

# Load data
filename = list(uploaded.keys())[0]  # Assuming only one file is uploaded
data = pd.read_csv(io.BytesIO(uploaded[filename]))

# Print column names
print("Column names:", data.columns)

X = data[['Theoretical pI', 'Instability index', 'Gravy']].values  # Assuming these are the features
y = data['Target'].values

# Shuffle the data
X, y = shuffle(X, y, random_state=42)

# Split data into train, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Reshape the input data to include the number of time steps
X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])
X_val = X_val.reshape(X_val.shape[0], 1, X_val.shape[1])
X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])

# Fit and transform the target values
y_train = label_encoder.fit_transform(y_train)
y_val = label_encoder.transform(y_val)
y_test = label_encoder.transform(y_test)

# Convert target values to float32
y_train = y_train.astype(np.float32)
y_val = y_val.astype(np.float32)
y_test = y_test.astype(np.float32)

# Define the input shape
input_shape = X_train.shape[1:]  # Shape of a single sample in X_train

print("X_train shape:", X_train.shape)
print("X_val shape:", X_val.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_val shape:", y_val.shape)
print("y_test shape:", y_test.shape)

# Build the model
model = Sequential([
    LSTM(64, activation='relu', return_sequences=True, input_shape=input_shape),
    Dropout(0.2),
    BatchNormalization(),
    SimpleRNN(64, activation='relu', return_sequences=True),
    Dropout(0.2),
    BatchNormalization(),
    tf.keras.layers.Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.2),
    BatchNormalization(),
    Dense(64, activation='relu'),
    Dropout(0.2),
    BatchNormalization(),
    Dense(1, activation='sigmoid')
])

# Compile the model
optimizer = Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', Precision(), Recall(), F1Score()])

# Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min')

# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val),
                    callbacks=[early_stopping, model_checkpoint])

# Load the best model
model.load_weights('best_model.h5')

# Evaluate the model
test_loss, test_acc, test_precision, test_recall, test_f1_score = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1_score}")

# Prediction example
new_data = np.random.random((1, 1, 3))
prediction = model.predict(new_data)
print("Prediction:", prediction)



results Test Accuracy: 0.540942907333374, Precision: 0.8327645063400269, Recall: 0.960629940032959, F1 Score: 0.8921388387680054