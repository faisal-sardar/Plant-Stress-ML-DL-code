import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, SimpleRNN, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.utils import shuffle
import numpy as np
import pandas as pd

# Load the data
data = pd.read_csv('stress.csv')

# Extract features and target
X = data[['Theoretical pI', 'Instability index', 'Gravy']].values
y = data['Target'].values

# Encode target labels
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Shuffle the data
X, y = shuffle(X, y, random_state=42)

# Split the data into train, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Reshape the input data to 3D tensor
X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))
X_val = np.reshape(X_val, (X_val.shape[0], 1, X_val.shape[1]))
X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))

# Define the model architecture
model = Sequential([
    LSTM(128, activation='relu', return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),
    Dropout(0.2),
    BatchNormalization(),
    LSTM(64, activation='relu', return_sequences=True),
    Dropout(0.2),
    BatchNormalization(),
    LSTM(32, activation='relu', return_sequences=True),
    Dropout(0.2),
    BatchNormalization(),
    SimpleRNN(16, activation='relu', return_sequences=True),
    Dropout(0.2),
    BatchNormalization(),
    tf.keras.layers.Flatten(),
    Dense(64, activation='relu'),
    Dropout(0.2),
    BatchNormalization(),
    Dense(1, activation='sigmoid')
])

# Compile the model
optimizer = Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

# Define callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min')

# Train the model
history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_val, y_val),
                    callbacks=[early_stopping, model_checkpoint])

# Load the best model
model.load_weights('best_model.h5')

# Evaluate the model
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc}")

# Calculate other performance measures
y_pred_prob = model.predict(X_test)
y_pred = (y_pred_prob > 0.5).astype(int)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")

# Example prediction
new_data = np.random.random((1, 1, 3))
prediction = model.predict(new_data)
print("Prediction:", prediction)



results 

Test Accuracy: 0.5111662745475769
13/13 [==============================] - 1s 5ms/step
Precision: 0.4581808565239531
Recall: 0.511166253101737
F1 Score: 0.4363587892588369